---
title: "PML Course Project - Prediction Assignment Writeup"
author: "Taiguara Tupinamb√°s"
date: "August 17, 2018"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

```{r packages}
library('caret')
library('rpart')
```

This course project aims at the prediction of the manner in which a person did an exercise. The dataset used was made available by the authors of following paper

\textit{Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.}

First the data loaded was preprocessed, by standardization and principal components analysis, and two prediction models were trained: Random Forest and Decision Trees. The one that obtained the lowest out-of-sample error, which was the former, was used on the testing data.


```{r data loading}
trainData <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testData <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

trainingData <- read.csv(trainData)
testingData <- read.csv(testData)

inTrain <- createDataPartition(y=trainingData$classe, 
                               p=0.7, list=FALSE)

training <- trainingData[inTrain,]
testing <- trainingData[-inTrain,]
```

## Exploratory Analysis

```{r cars}
dim(training)
```

The data from HAR has 160 columns, from which a lot of them should not be used by the models. We begin by removing columns that has NA values, that serves as identification or timestamp record and that has no significant variation and therefore would not help with model prediction (near-zero variance).
All this data preprocessing is done looking at the training partition, and applied equally to the testing partition.

```{r preprocess}

# Selecting only columns with less than 1% of its values as NA
naColumns <- sapply(training, function(c) mean(is.na(c)))<0.05
training <- training[, naColumns]
testing <- testing[, naColumns]
testingData <-testingData[,naColumns]


# Removing first 5 columns of identification
training <- training[,-(1:5)]
testing <- testing[, -(1:5)]
testingData <-testingData[,-(1:5)]

# Removing near-zero value columns
nzv <- nearZeroVar(training)
training <- training[, -nzv]
testing <- testing[, -nzv]
testingData <-testingData[,-nzv]

dim(training)
```

After the first cleanse, there are 49 variables that can be used to predict the class of activity.
The next step, to improve model design, is to standardize the data

``` {r standardization}
preObj <- preProcess(training[,names(training)!="classe"], method=c("center","scale"))
trainStd <- predict(preObj,training[,names(training)!="classe"])
testStd <- predict(preObj,testing[,names(testing)!="classe"])

testingDataStd <- predict(preObj,testingData[,names(testing)!="problem_id"])
```


To check if further dimensionality reduction is necessary, we perform a correlation analysis

``` {r coranalysis}
#calculate correlation between variables
M <- abs(cor(training[,names(training)!="classe"]))
diag(M) <- 0
#printing those variables that are highly correlated
which(M > 0.8,arr.ind=T)
```

As we can see, there are a lot of variables pairs with high correlation. Therefore we will use Principal Components Analysis, to reduce dimensionality.

``` {r pca}
preProc <- preProcess(trainStd, 
                      method="pca", thresh = 0.9)
trainPC <- predict(preProc,trainStd)
testPC <- predict(preProc,testStd)

testingDataPC <- predict(preProc,testingDataStd)

```

## Model Design

Two models are built and then the one with the lowest out of sample error will be chosen as final model. 
First a random forest is constructed on the processed data set.

```{r randomForest}
set.seed(1313)
trainPC['classe'] <- training$classe

modRf <- train(classe ~., method="rf",data=trainPC)
predRf <- predict(modRf,testPC)
confusionMatrix(predRf,testing$classe)

```


```{r decisionTree}
set.seed(1313)
# modDt <- train(training$classe ~., method="rpart",data=trainPC)
modDt <-  rpart(classe~., data=trainPC, method="class")
predDt <- predict(modDt, newdata=testPC, type="class") 
confusionMatrix(predDt,testing$classe)  
```

```{r outofsampleerror}

outOfSampleErrorRF <- 1 - as.numeric(confusionMatrix(predRf,testing$classe)$overall[1])
outOfSampleErrorDT <- 1 - as.numeric(confusionMatrix(predDt,testing$classe)$overall[1])

outOfSampleErrorRF
outOfSampleErrorDT
```

The out-of-sample error obtained by the Ranfom Forest is much lower, approximately 2.3%. Therefore it will be the model used for the final testing set.

```{r results}

predTestingData <- predict(modRf,testingDataPC)
predTestingData
```
